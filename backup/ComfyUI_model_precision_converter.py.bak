import torch
import os
import folder_paths
import pickle
from safetensors.torch import load_file as load_safetensors, save_file as save_safetensors
from safetensors import SafetensorError
from comfy.utils import load_torch_file


class ModelPrecisionConverter:
    """
    模型精度转换节点，支持模型修复、精度转换，支持bf16、fp16、fp32、int4、int8模型互转，支持单文件和diffusers格式
    """
    def __init__(self):
        self.output_dir = os.path.join(folder_paths.models_dir, "converted_models")
        self.repair_dir = os.path.join(folder_paths.models_dir, "repaired_models")
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.repair_dir, exist_ok=True)
        
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_type": (["single_file", "diffusers"], {
                    "default": "single_file",
                    "tooltip": "选择模型类型：single_file(单文件模型)或diffusers(扩散模型目录)"
                }),
                "model_path": ("STRING", {
                    "default": "", 
                    "placeholder": "输入要转换的模型路径(文件或目录)",
                    "forceInput": True
                }),
                "input_precision": (["fp32","bf16", "fp16","float8_e4m3fn", "int8","int4"], {
                    "default": "bf16",
                    "tooltip": "输入模型的原始精度"
                }),
                "target_precision": (["fp32","bf16", "fp16","float8_e4m3fn", "int8","int4"], {
                    "default": "float8_e4m3fn",
                    "tooltip": "目标精度，需从列表中选择"
                }),
                "save_directory": ("STRING", {
                    "default": "", 
                    "placeholder": "输入保存目录，留空使用默认目录",
                    "forceInput": True
                }),
                "quantization_scheme": (["dynamic", "static"], {
                    "default": "dynamic",
                    "tooltip": "动态量化(推荐)或静态量化，仅对int4/int8有效"
                }),
            },
            "optional": {
                "repair_model": ("BOOLEAN", {
                    "default": False,
                    "label_on": "启用修复",
                    "label_off": "禁用修复"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("output_path", "repair_info")
    FUNCTION = "process_model"
    CATEGORY = "模型工具/精度转换与修复"
    
    def process_model(self, model_type, model_path, input_precision, target_precision, save_directory, 
                     quantization_scheme, repair_model=False):
        # 验证参数有效性
        valid_target_precisions = ["float8_e4m3fn", "fp16", "fp32", "int4", "int8"]
        valid_quant_schemes = ["dynamic", "static"]
        
        if target_precision not in valid_target_precisions:
            raise ValueError(f"无效的目标精度: {target_precision}，必须是{valid_target_precisions}之一")
        
        if quantization_scheme not in valid_quant_schemes:
            raise ValueError(f"无效的量化方案: {quantization_scheme}，必须是{valid_quant_schemes}之一")
        
        # 验证模型路径
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"模型路径不存在: {model_path}")
            
        # 根据模型类型处理
        if model_type == "diffusers":
            return self._process_diffusers_model(
                model_path, input_precision, target_precision, save_directory,
                quantization_scheme, repair_model
            )
        else:
            return self._process_single_file_model(
                model_path, input_precision, target_precision, save_directory,
                quantization_scheme, repair_model
            )
    
    def _process_single_file_model(self, model_path, input_precision, target_precision, save_directory, 
                                 quantization_scheme, repair_model=False):
        # 修复模型（如果启用）
        repair_info = "未进行修复"
        if repair_model:
            try:
                model_path, repair_details = self._repair_model(model_path)
                repair_info = f"修复成功: {repair_details}"
            except Exception as e:
                repair_info = f"修复失败: {str(e)}"
        
        # 确定保存目录
        if not save_directory:
            save_directory = self.output_dir
        os.makedirs(save_directory, exist_ok=True)
        
        # 加载模型
        try:
            model = self._load_model(model_path)
        except Exception as e:
            raise Exception(f"加载模型失败: {str(e)}")
        
        # 转换精度
        try:
            converted_model = self._convert_precision(
                model, 
                input_precision, 
                target_precision,
                quantization_scheme
            )
        except Exception as e:
            raise Exception(f"转换精度失败: {str(e)}")
        
        # 生成输出文件名
        base_name = os.path.basename(model_path)
        name, ext = os.path.splitext(base_name)
        output_filename = f"{name}_{target_precision}{ext}"
        output_path = os.path.join(save_directory, output_filename)
        
        # 保存转换后的模型
        try:
            self._save_model(converted_model, output_path)
        except Exception as e:
            raise Exception(f"保存模型失败: {str(e)}")
        
        # 格式化结果展示（始终显示）
        result_display = self._format_result_display(
            model_path, output_path, input_precision, target_precision, repair_info
        )
            
        return (output_path, repair_info, result_display)
    
    def _process_diffusers_model(self, model_path, input_precision, target_precision, save_directory,
                                quantization_scheme, repair_model=False):
        """处理diffusers格式模型目录"""
        #from diffusers import StableDiffusionPipeline   , DPTForDepthEstimation
        from diffusers import StableDiffusionPipeline  
        from diffusers.utils import is_safetensors_available
        
        # 确定保存目录
        if not save_directory:
            save_directory = os.path.join(self.output_dir, f"diffusers_{target_precision}")
        os.makedirs(save_directory, exist_ok=True)
        
        repair_info = "未进行修复"
        if repair_model:
            try:
                model_path, repair_details = self._repair_diffusers_model(model_path)
                repair_info = f"修复成功: {repair_details}"
            except Exception as e:
                repair_info = f"修复失败: {str(e)}"
        
        # 获取torch dtype
        def get_torch_dtype(precision):
            dtype_map = {
                "bf16": torch.bfloat16,
                "fp16": torch.float16,
                "fp32": torch.float32,
                "float8_e4m3fn": torch.float8_e4m3fn,
            }
            return dtype_map.get(precision, torch.float32)
        
        # 加载diffusers模型
        try:
            # 尝试加载 Stable Diffusion 管道
            pipeline = StableDiffusionPipeline.from_pretrained(
                model_path,
                torch_dtype=get_torch_dtype(input_precision),
                safety_checker=None,
                #device_map="cpu"
                device_map="balanced" 
            )
            
            
       
        except  Exception as e:
            raise Exception(f"加载diffusers模型失败: {str(e)}")
            """
            # 尝试加载其他类型模型
            try:
                pipeline = DPTForDepthEstimation.from_pretrained(
                    model_path,
                    torch_dtype=get_torch_dtype(input_precision),
                    device_map="auto" 
                )
            except Exception as e:
                raise Exception(f"加载diffusers模型失败: {str(e)}")
            """
        # 转换精度
        target_dtype = get_torch_dtype(target_precision)
        
        # 处理量化（仅对整数精度有效）
        if target_precision in ["int4", "int8"]:
            try:
                if target_precision == "int8":
                    pipeline = pipeline.quantize(8, quantization_scheme=quantization_scheme)
                else:  # int4
                    pipeline = pipeline.quantize(4, quantization_scheme=quantization_scheme)
            except Exception as e:
                raise Exception(f"模型量化失败: {str(e)}")
        else:
            # 浮点精度转换
            pipeline.to(dtype=target_dtype)
        
        # 保存转换后的模型
        try:
            pipeline.save_pretrained(
                save_directory,
                safe_serialization=is_safetensors_available()
            )
        except Exception as e:
            raise Exception(f"保存diffusers模型失败: {str(e)}")
        
        # 格式化结果展示（始终显示）
        result_display = self._format_result_display(
            model_path, save_directory, input_precision, target_precision, repair_info
        )
            
        return (save_directory, repair_info, result_display)
    
    def _repair_model(self, model_path):
        """修复损坏的单文件模型，支持safetensors和ckpt格式"""
        file_ext = os.path.splitext(model_path)[1].lower()
        base_name = os.path.basename(model_path)
        repair_path = os.path.join(self.repair_dir, f"repaired_{base_name}")
        
        # 修复safetensors文件
        if file_ext == ".safetensors":
            try:
                data = load_safetensors(model_path, device="cpu")
                save_safetensors(data, repair_path)
                return repair_path, "Safetensors文件重新序列化修复"
            except SafetensorError as e:
                if "HeaderTooLarge" in str(e):
                    return self._repair_large_header(model_path, repair_path)
                else:
                    return self._convert_to_ckpt(model_path, repair_path), "Safetensors转CKPT修复"
        
        # 修复ckpt文件
        elif file_ext in [".ckpt", ".pt"]:
            try:
                data = torch.load(model_path, map_location="cpu")
                torch.save(data, repair_path)
                return repair_path, "CKPT文件重新序列化修复"
            except Exception as e:
                return self._convert_to_safetensors(model_path, repair_path), "CKPT转Safetensors修复"
        
        raise Exception(f"不支持的文件格式: {file_ext}")
    
    def _repair_diffusers_model(self, model_path):
        """修复损坏的diffusers模型目录"""
        import shutil
        
        # 检查必要文件
        required_files = ["config.json", "model_index.json"]
        missing_files = []
        
        for file in required_files:
            if not os.path.exists(os.path.join(model_path, file)):
                missing_files.append(file)
        
        if missing_files:
            raise Exception(f"缺少必要的diffusers文件: {', '.join(missing_files)}")
        
        # 创建修复目录
        base_name = os.path.basename(model_path)
        repair_path = os.path.join(self.repair_dir, f"repaired_{base_name}")
        os.makedirs(repair_path, exist_ok=True)
        
        # 复制并修复文件
        for root, dirs, files in os.walk(model_path):
            rel_path = os.path.relpath(root, model_path)
            target_dir = os.path.join(repair_path, rel_path)
            os.makedirs(target_dir, exist_ok=True)
            
            for file in files:
                src = os.path.join(root, file)
                dst = os.path.join(target_dir, file)
                
                if file.endswith(".safetensors"):
                    try:
                        data = load_safetensors(src, device="cpu")
                        save_safetensors(data, dst)
                    except:
                        shutil.copy2(src, dst)
                elif file.endswith((".ckpt", ".pt")):
                    try:
                        data = torch.load(src, map_location="cpu")
                        torch.save(data, dst)
                    except:
                        shutil.copy2(src, dst)
                else:
                    shutil.copy2(src, dst)
        
        return repair_path, "Diffusers目录文件检查与重新序列化"
    
    def _repair_large_header(self, model_path, repair_path):
        """修复头部过大的safetensors文件"""
        from safetensors.header import Header
        
        with open(model_path, "rb") as f:
            header = Header.from_file(f)
            f.seek(0)
            data = f.read()
        
        # 简化元数据以减小头部大小
        simplified_metadata = {
            "format": "safetensors",
            "version": header.metadata.get("version", "1.0")
        }
        
        new_header = Header(
            metadata=simplified_metadata,
            tensors=header.tensors
        )
        
        with open(repair_path, "wb") as f:
            new_header.write(f)
            f.write(data[header.header_size:])
        
        return repair_path, "过大头部修复（简化元数据）"
    
    def _convert_to_ckpt(self, model_path, repair_path):
        """将safetensors转换为ckpt格式"""
        data = load_safetensors(model_path, device="cpu")
        output_path = repair_path.replace(".safetensors", ".ckpt")
        torch.save(data, output_path)
        return output_path
    
    def _convert_to_safetensors(self, model_path, repair_path):
        """将ckpt转换为safetensors格式"""
        data = torch.load(model_path, map_location="cpu")
        output_path = repair_path.replace(".ckpt", ".safetensors")
        save_safetensors(data, output_path)
        return output_path
    
    def _load_model(self, model_path):
        """增强的模型加载函数，支持多种格式"""
        file_ext = os.path.splitext(model_path)[1].lower()
        
        try:
            if file_ext == ".safetensors":
                return load_safetensors(model_path, device="cpu")
            elif file_ext in [".ckpt", ".pt"]:
                return torch.load(model_path, map_location="cpu", weights_only=True)
            else:
                return load_torch_file(model_path)
        except Exception as e:
            try:
                with open(model_path, "rb") as f:
                    return pickle.load(f)
            except:
                raise e
    
    def _save_model(self, model, path):
        """自定义模型保存函数，支持多种格式"""
        file_ext = os.path.splitext(path)[1].lower()
        has_dicts = any(isinstance(v, dict) for v in model.values())
        
        if file_ext == ".safetensors" and not has_dicts:
            save_safetensors(model, path)
        elif file_ext in [".ckpt", ".pt"]:
            torch.save(model, path)
        else:
            with open(path, "wb") as f:
                pickle.dump(model, f)
    
    def _convert_precision(self, model, input_precision, target_precision, quantization_scheme):
        """转换模型权重精度"""
        precision_map = {
            "bf16": torch.bfloat16,
            "fp16": torch.float16,
            "fp32": torch.float32,
            "float8_e4m3fn": torch.float8_e4m3fn,
            "int4": torch.int4,
            "int8": torch.int8
        }
        
        # 整数精度反量化
        if input_precision in ["int4", "int8"]:
            model = self._dequantize_model(model, input_precision)
        
        # 整数精度转换
        if target_precision in ["int4", "int8"]:
            return self._integer_quantization(
                model, 
                input_precision, 
                target_precision,
                quantization_scheme
            )
        
        # 浮点精度转换
        target_dtype = precision_map[target_precision]
        converted = {}
        for k, v in model.items():
            if isinstance(v, torch.Tensor):
                converted[k] = v.to(torch.float32).to(target_dtype)
            else:
                converted[k] = v
                
        return converted
    
    def _dequantize_model(self, model, input_precision):
        """将量化模型反量化为float32"""
        dequantized = {}
        for k, v in model.items():
            if isinstance(v, torch.Tensor) and v.dtype in [torch.int4, torch.int8]:
                dequantized[k] = v.to(torch.float32)
            elif isinstance(v, dict) and "dtype" in v and v["dtype"] in ["int4", "int8"]:
                dequantized[k] = torch.tensor(v["data"], dtype=torch.float32)
            else:
                dequantized[k] = v
        return dequantized
    
    def _integer_quantization(self, model, input_precision, target_precision, scheme):
        """对模型进行整数量化"""
        target_dtype = torch.int4 if target_precision == "int4" else torch.int8
        quantized = {}
        
        for k, v in model.items():
            if isinstance(v, torch.Tensor) and v.dtype.is_floating_point:
                float_tensor = v.to(torch.float32)
                
                if scheme == "dynamic":
                    quantized_tensor = torch.quantize_per_tensor(
                        float_tensor,
                        scale=1.0,
                        zero_point=0,
                        dtype=target_dtype
                    )
                else:  # static
                    quantized_tensor = torch.quantize_per_tensor(
                        float_tensor,
                        scale=float_tensor.abs().max() / ((1 << (target_dtype.itemsize * 8 - 1)) - 1),
                        zero_point=0,
                        dtype=target_dtype
                    )
                
                quantized[k] = quantized_tensor
            else:
                quantized[k] = v
                
        return quantized
    
    def _format_result_display(self, input_path, output_path, input_precision, target_precision, repair_info):
        """格式化结果展示文本"""
        return (
            f"模型转换完成!\n"
            f"源路径: {input_path}\n"
            f"目标路径: {output_path}\n"
            f"精度转换: {input_precision} → {target_precision}\n"
            f"修复状态: {repair_info}\n"
            f"提示: 可右键执行此节点查看最新结果"
        )
