# 模型精度转换工具 - ComfyUI节点

这是一个轻量高效的ComfyUI自定义节点，支持主流精度模型之间的双向转换，包括浮点型与整数型精度的互转，帮助灵活优化模型体积、内存占用和推理效率，适配大多数深度学习场景。

## 功能特点

- **输入精度支持**：bf16、fp16、fp32、int4、int8（支持整数精度模型作为输入）
- **目标精度支持**：fp8、fp16、fp32、int4、int8（可将任意输入精度转换为目标精度）
- **双向转换能力**：支持浮点转整数（如fp32→int8）、整数转浮点（如int4→fp16）、同类型精度互转（如int4→int8）
- **自动化处理**：自动生成带精度后缀的输出文件名（如`_int4`、`_fp8`），自动创建输出目录
- **灵活量化方案**：针对整数精度提供动态量化（易用）和静态量化（高精度）两种选择

## 安装方法

1. 将`model_precision_converter.py`文件复制到ComfyUI的`custom_nodes`目录下
2. 重启ComfyUI
3. 节点将出现在"模型工具/精度转换"分类中

## 使用方法

1. 在ComfyUI工作流中添加"模型精度转换工具"节点
2. 配置参数：
   - `model_path`：输入要转换的模型文件路径（支持本地任意路径）
   - `input_precision`：选择原始模型的精度（bf16、fp16、fp32、int4、int8） 
   - `target_precision`：选择目标精度（fp8、fp16、fp32、int4、int8）
   - `save_directory`：指定保存目录（留空则使用默认目录`models/converted_models`）
   - `quantization_scheme`：整数量化方案（仅对目标精度为int4/int8生效，可选dynamic/static）
3. 运行工作流，节点返回转换后模型的保存路径

## 支持的精度类型说明

| 精度类型 | 描述 | 适用场景 |
|---------|------|---------|
| bf16    | 16位脑浮点数 | 训练/推理中间精度，平衡精度与性能 |
| fp16    | 16位浮点数 | 主流中等精度推理格式，兼容性最广 |
| fp32    | 32位浮点数 | 标准精度，训练默认格式，精度最高 |
| fp8     | 8位浮点数（e4m3格式） | 高效推理，显存占用低（仅fp32的1/4） |
| int8    | 8位整数 | 常用量化格式，精度损失小，压缩比4:1 |
| int4    | 4位整数 | 高压缩比（8:1），适合显存受限场景 |

## 核心功能：双向转换能力

| 转换类型 | 典型场景 | 工具处理逻辑 |
|---------|---------|------------|
| 浮点→整数（如fp32→int8） | 模型压缩、加速推理 | 量化处理：计算缩放因子和零点，将浮点值映射到整数范围 |
| 整数→浮点（如int4→fp16） | 精度恢复、模型微调 | 反量化处理：通过存储的缩放因子和零点，将整数还原为浮点值 |
| 浮点→浮点（如bf16→fp8） | 精度调整、适配硬件 | 直接类型转换，保留数值范围 |
| 整数→整数（如int8→int4） | 极致压缩 | 先反量化为浮点，再重新量化为目标整数精度 |

## 量化方案说明

1. **动态量化（dynamic）**
   - 无需校准数据，一键转换，适合快速测试
   - 实时计算张量的数值范围并生成缩放因子
   - 优势：易用性高，兼容性好；劣势：精度略低于静态量化

2. **静态量化（static）**
   - 需校准数据（实际使用中需额外准备）以优化精度
   - 基于校准数据预计算最优缩放因子和零点
   - 优势：精度更高；劣势：需额外数据，流程较复杂

## 技术细节

- **浮点转换**：基于PyTorch原生API（如`to(torch.float8_e4m3fn)`），确保转换稳定性
- **整数转换**：
  - 量化：通过缩放因子（scale）和零点（zero_point）将浮点值映射到整数区间
  - 反量化：加载整数模型时，自动使用存储的scale和zero_point还原为浮点值
- **数据存储**：整数模型以字典格式存储量化后的数据、scale和zero_point，确保转换可逆

## 注意事项
- 转化.savetensors模型，comfyUI 提示确认是否是ckpt，要求改名为.ckpt，.pt再试时，
可以改模型名.ckpt尝试,测试使用了sd1.5文生图anything_fp16.savetensors模型转化fp8_e4m3fn，
转换后改名anything_fp8.ckpt,正常出图。

- 整数→浮点转换可能因原量化损失导致精度无法完全恢复
- 转换大模型时建议关闭其他程序，避免内存不足
- 模型体积参考（相对于fp32）：
  - fp16/bf16：50%；fp8/int8：25%；int4：12.5%
- 依赖环境：PyTorch 2.0+（确保fp8和整数量化功能正常）
- 整数模型需通过本工具或兼容的反量化逻辑加载，否则可能无法正确使用

## 常见问题

1. **整数模型转换失败**
   - 确保输入精度选择正确（int4/int8需匹配模型实际精度）
   - 检查模型文件是否为工具生成的量化格式（含scale和zero_point）

2. **转换后模型精度下降明显**
   - 尝试更高精度的目标格式（如int4→int8）
   - 整数量化时改用静态量化并提供校准数据

3. **保存路径无输出文件**
   - 检查路径权限，确保有写入权限
   - 确认模型转换过程未报错（查看ComfyUI控制台日志）

## 许可证

本项目采用MIT许可证，详情请见LICENSE文件。